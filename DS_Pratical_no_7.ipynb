{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB5QnS/0aE9MWLd44vvFjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlanetDestroyyer/DS-Pratical-TE-AIDS/blob/main/DS_Pratical_no_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwSCC6fPcaNQ",
        "outputId": "93582f5c-59d9-4e42-ec8e-bc92f413e59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language. It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way. NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval. Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.\"\"\""
      ],
      "metadata": {
        "id": "NBO9oUjudf7C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(document)"
      ],
      "metadata": {
        "id": "RWe5KlVdehna"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(tokens)"
      ],
      "metadata": {
        "id": "hz9M5_BzenJe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "rZyJhxJ_e46z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]"
      ],
      "metadata": {
        "id": "r8AZ5rs0fD0o"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]"
      ],
      "metadata": {
        "id": "KzVhH2kogKbn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]"
      ],
      "metadata": {
        "id": "maO5jlE_fpxZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Document:\\n\", document)\n",
        "print(\"\\nTokens:\\n\", tokens)\n",
        "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
        "print(\"\\nFiltered Tokens (after stop words removal):\\n\", filtered_tokens)\n",
        "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens)\n",
        "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzU4DyU0gRh8",
        "outputId": "ff55aec3-38dd-4e6f-ba6d-565f8c9f2522"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Document:\n",
            " Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans using natural language. It involves the analysis, understanding, and generation of human language, enabling machines to process and comprehend text in a meaningful way. NLP techniques are widely used in various applications such as sentiment analysis, machine translation, chatbots, and information retrieval. Preprocessing is an essential step in NLP, which involves tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization.\n",
            "\n",
            "Tokens:\n",
            " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'It', 'involves', 'the', 'analysis', ',', 'understanding', ',', 'and', 'generation', 'of', 'human', 'language', ',', 'enabling', 'machines', 'to', 'process', 'and', 'comprehend', 'text', 'in', 'a', 'meaningful', 'way', '.', 'NLP', 'techniques', 'are', 'widely', 'used', 'in', 'various', 'applications', 'such', 'as', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'and', 'information', 'retrieval', '.', 'Preprocessing', 'is', 'an', 'essential', 'step', 'in', 'NLP', ',', 'which', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
            "\n",
            "POS Tags:\n",
            " [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('using', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('the', 'DT'), ('analysis', 'NN'), (',', ','), ('understanding', 'NN'), (',', ','), ('and', 'CC'), ('generation', 'NN'), ('of', 'IN'), ('human', 'JJ'), ('language', 'NN'), (',', ','), ('enabling', 'VBG'), ('machines', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('comprehend', 'VB'), ('text', 'NN'), ('in', 'IN'), ('a', 'DT'), ('meaningful', 'JJ'), ('way', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('techniques', 'NNS'), ('are', 'VBP'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('various', 'JJ'), ('applications', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('machine', 'NN'), ('translation', 'NN'), (',', ','), ('chatbots', 'NNS'), (',', ','), ('and', 'CC'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.'), ('Preprocessing', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('essential', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('NLP', 'NNP'), (',', ','), ('which', 'WDT'), ('involves', 'VBZ'), ('tokenization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n",
            "\n",
            "Filtered Tokens (after stop words removal):\n",
            " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focuses', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'involves', 'analysis', ',', 'understanding', ',', 'generation', 'human', 'language', ',', 'enabling', 'machines', 'process', 'comprehend', 'text', 'meaningful', 'way', '.', 'NLP', 'techniques', 'widely', 'used', 'various', 'applications', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'information', 'retrieval', '.', 'Preprocessing', 'essential', 'step', 'NLP', ',', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n",
            "\n",
            "Stemmed Tokens:\n",
            " ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'artifici', 'intellig', '(', 'ai', ')', 'focus', 'interact', 'comput', 'human', 'use', 'natur', 'languag', '.', 'involv', 'analysi', ',', 'understand', ',', 'gener', 'human', 'languag', ',', 'enabl', 'machin', 'process', 'comprehend', 'text', 'meaning', 'way', '.', 'nlp', 'techniqu', 'wide', 'use', 'variou', 'applic', 'sentiment', 'analysi', ',', 'machin', 'translat', ',', 'chatbot', ',', 'inform', 'retriev', '.', 'preprocess', 'essenti', 'step', 'nlp', ',', 'involv', 'token', ',', 'part-of-speech', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.']\n",
            "\n",
            "Lemmatized Tokens:\n",
            " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', '(', 'AI', ')', 'focus', 'interaction', 'computer', 'human', 'using', 'natural', 'language', '.', 'involves', 'analysis', ',', 'understanding', ',', 'generation', 'human', 'language', ',', 'enabling', 'machine', 'process', 'comprehend', 'text', 'meaningful', 'way', '.', 'NLP', 'technique', 'widely', 'used', 'various', 'application', 'sentiment', 'analysis', ',', 'machine', 'translation', ',', 'chatbots', ',', 'information', 'retrieval', '.', 'Preprocessing', 'essential', 'step', 'NLP', ',', 'involves', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFEW1G_tgzUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B"
      ],
      "metadata": {
        "id": "mCeE_i0Xg3ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "btbxxKC1g4hc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"the sky is blue\",\n",
        "    \"the sun is bright\",\n",
        "    \"the sun in the blue sky is bright\"\n",
        "]"
      ],
      "metadata": {
        "id": "oMxz7CrihIzk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "h666TJw2jzuO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "XWUIZ3CTkjr9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_name = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "dSFnj8ssksTN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,doc in enumerate(documents):\n",
        "  print(f\"TF-IDF vector for document {i+1}: {doc}\")\n",
        "  for j,word in enumerate(feature_name):\n",
        "    print(f\"{word}: {tfidf_matrix[i,j]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knq8zMwgkvAD",
        "outputId": "035c777a-6120-4a3c-a797-1de341d4f510"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vector for document 1: the sky is blue\n",
            "blue: 0.5584778353707552\n",
            "bright: 0.0\n",
            "in: 0.0\n",
            "is: 0.4337078595086741\n",
            "sky: 0.5584778353707552\n",
            "sun: 0.0\n",
            "the: 0.4337078595086741\n",
            "TF-IDF vector for document 2: the sun is bright\n",
            "blue: 0.0\n",
            "bright: 0.5584778353707552\n",
            "in: 0.0\n",
            "is: 0.4337078595086741\n",
            "sky: 0.0\n",
            "sun: 0.5584778353707552\n",
            "the: 0.4337078595086741\n",
            "TF-IDF vector for document 3: the sun in the blue sky is bright\n",
            "blue: 0.33817065011441144\n",
            "bright: 0.33817065011441144\n",
            "in: 0.4446537658935842\n",
            "is: 0.26261967713080414\n",
            "sky: 0.33817065011441144\n",
            "sun: 0.33817065011441144\n",
            "the: 0.5252393542616083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hi7SIfF0kxCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}